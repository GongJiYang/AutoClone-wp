#!/usr/bin/env python3
"""
é¡µé¢åˆ†æè„šæœ¬ - ä¼˜åŒ–ç‰ˆ
æ•´åˆè§†è§‰åˆ†æå’ŒWordPressä»£ç ç”Ÿæˆ

ç”¨æ³•:
  python scripts/analyze.py <url> [output_name]
  
ç¤ºä¾‹:
  python scripts/analyze.py https://example.com/products
  python scripts/analyze.py https://example.com/products my-products
"""
import asyncio
import json
import re
import shutil
import sys
import time
from pathlib import Path
from typing import Optional, Tuple
from urllib.parse import urlparse

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from playwright.async_api import async_playwright  # type: ignore
from src.core.config import Config
from src.core.analyzer import VisionAnalyzer, generate_cursor_prompt_file

# ============== å¸¸é‡é…ç½® ==============
# æ»šåŠ¨å’Œç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
SCROLL_WAIT_MS = 500
ANIMATION_WAIT_MS = 400
LAZY_LOAD_WAIT_MS = 1500  # å¢åŠ æ‡’åŠ è½½ç­‰å¾…æ—¶é—´
FIRST_SCREEN_WAIT_MS = 2500  # å¢åŠ ç¬¬ä¸€å±ç­‰å¾…æ—¶é—´
ANIMATION_COMPLETE_WAIT_MS = 3000
ELEMENT_SCREENSHOT_TIMEOUT_MS = 10000
IMAGE_LOAD_CHECK_TIMEOUT_MS = 5000  # å›¾ç‰‡åŠ è½½æ£€æŸ¥è¶…æ—¶
IMAGE_LOAD_RETRY_DELAY_MS = 200  # å›¾ç‰‡åŠ è½½é‡è¯•é—´éš”

# API è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
API_RATE_LIMIT_DELAY = 1


def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
    
    Args:
        name: åŸå§‹æ–‡ä»¶å
        
    Returns:
        str: å®‰å…¨çš„æ–‡ä»¶å
    """
    # åªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦
    safe_name = re.sub(r'[^\w\-]', '_', name)
    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
    safe_name = re.sub(r'_+', '_', safe_name)
    # ç§»é™¤é¦–å°¾ä¸‹åˆ’çº¿
    safe_name = safe_name.strip('_')
    return safe_name or 'page'


def extract_domain_name(url: str) -> str:
    """ä»URLæå–åŸŸåä½œä¸ºå½’æ¡£ç›®å½•å
    
    Args:
        url: ç½‘é¡µURL
        
    Returns:
        str: æ¸…ç†åçš„åŸŸåï¼ˆç”¨äºç›®å½•åï¼‰
    """
    try:
        parsed = urlparse(url)
        domain = parsed.netloc or parsed.path
        # ç§»é™¤ www. å‰ç¼€
        domain = re.sub(r'^www\.', '', domain)
        # ç§»é™¤ç«¯å£å·
        domain = domain.split(':')[0]
        # æ¸…ç†ä¸ºå®‰å…¨çš„ç›®å½•å
        return sanitize_filename(domain)
    except Exception:
        return sanitize_filename(url.rstrip('/').split('/')[-1] or 'unknown')


def archive_files(url: str, output_name: str) -> None:
    """å°†æ–‡ä»¶å¤åˆ¶åˆ°å½’æ¡£ç›®å½•ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰
    
    Args:
        url: ç½‘é¡µURL
        output_name: è¾“å‡ºæ–‡ä»¶å
    """
    domain_name = extract_domain_name(url)
    archive_base_dir = Config.ARCHIVE_DIR / domain_name
    archive_base_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•
    archive_screenshots_dir = archive_base_dir / "screenshots"
    archive_analysis_dir = archive_base_dir / "analysis"
    archive_screenshots_dir.mkdir(parents=True, exist_ok=True)
    archive_analysis_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“¦ å¤åˆ¶æ–‡ä»¶åˆ°å½’æ¡£ç›®å½•: {archive_base_dir}...", flush=True)
    
    archived_count = 0
    archived_size = 0
    
    # å¤åˆ¶æˆªå›¾æ–‡ä»¶ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰
    for pattern in [f"{output_name}_*.jpg", f"{output_name}_*.png"]:
        for file_path in Config.SCREENSHOT_DIR.glob(pattern):
            try:
                dest_path = archive_screenshots_dir / file_path.name
                # å¦‚æœå½’æ¡£ç›®å½•å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼Œå…ˆåˆ é™¤
                if dest_path.exists():
                    dest_path.unlink()
                size = file_path.stat().st_size
                shutil.copy2(str(file_path), str(dest_path))
                archived_count += 1
                archived_size += size
            except Exception as e:
                print(f"   âš ï¸  å¤åˆ¶å¤±è´¥ {file_path.name}: {e}", flush=True)
    
    # å¤åˆ¶åˆ†ææ–‡ä»¶ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰
    for pattern in [f"{output_name}_*.json", f"{output_name}_*.md"]:
        for file_path in Config.ANALYSIS_DIR.glob(pattern):
            try:
                dest_path = archive_analysis_dir / file_path.name
                # å¦‚æœå½’æ¡£ç›®å½•å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼Œå…ˆåˆ é™¤
                if dest_path.exists():
                    dest_path.unlink()
                size = file_path.stat().st_size
                shutil.copy2(str(file_path), str(dest_path))
                archived_count += 1
                archived_size += size
            except Exception as e:
                print(f"   âš ï¸  å¤åˆ¶å¤±è´¥ {file_path.name}: {e}", flush=True)
    
    if archived_count > 0:
        print(f"   âœ… å·²å¤åˆ¶ {archived_count} ä¸ªæ–‡ä»¶åˆ°å½’æ¡£ç›®å½• ({archived_size / (1024*1024):.2f} MB)", flush=True)
        print(f"   ğŸ“ å½’æ¡£ä½ç½®: {archive_base_dir}", flush=True)
        print(f"   ğŸ“‚ åŸæ–‡ä»¶ä¿ç•™åœ¨: {Config.SCREENSHOT_DIR} å’Œ {Config.ANALYSIS_DIR}", flush=True)
    else:
        print(f"   âš ï¸  æ²¡æœ‰æ‰¾åˆ°è¦å½’æ¡£çš„æ–‡ä»¶", flush=True)


def get_class_name_safe(class_name) -> str:
    """å®‰å…¨è·å– classNameï¼Œå¤„ç† SVG å…ƒç´ ç­‰ç‰¹æ®Šæƒ…å†µ
    
    Args:
        class_name: å…ƒç´ çš„ className å±æ€§
        
    Returns:
        str: å®‰å…¨çš„ç±»åå­—ç¬¦ä¸²
    """
    if isinstance(class_name, str):
        return class_name[:100]
    return ''


async def detect_dom_sections(page) -> list[dict]:
    """æ£€æµ‹DOMç»“æ„ä¸­çš„sectionsï¼Œè¿”å›åŒ…å«é€‰æ‹©å™¨çš„ä¿¡æ¯
    
    Returns:
        list[dict]: åŒ…å«sectionä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ï¼š
            - tag: æ ‡ç­¾å
            - className: ç±»å
            - id: ID
            - selector: CSSé€‰æ‹©å™¨
            - top: é¡¶éƒ¨ä½ç½®
            - height: é«˜åº¦
            - bottom: åº•éƒ¨ä½ç½®
            - width: å®½åº¦
    """
    sections = await page.evaluate("""
        () => {
            const selectors = [
                'header', 'nav', 'main > section', 'section',
                '[class*="hero"]', '[class*="banner"]', '[class*="features"]',
                '[class*="products"]', '[class*="testimonials"]', '[class*="cta"]',
                '[class*="footer"]', 'footer', '[id*="section"]', '[class*="section"]'
            ];
            
            const found = new Set();
            const sections = [];
            
            // å®‰å…¨è·å– className çš„è¾…åŠ©å‡½æ•°
            const getClassName = (el) => {
                if (typeof el.className === 'string') {
                    return el.className;
                }
                // å¤„ç† SVG å…ƒç´ çš„ className (SVGAnimatedString)
                if (el.className && el.className.baseVal !== undefined) {
                    return el.className.baseVal;
                }
                return '';
            };
            
            for (const selector of selectors) {
                const elements = document.querySelectorAll(selector);
                elements.forEach((el, idx) => {
                    const rect = el.getBoundingClientRect();
                    const top = Math.round(rect.top + window.scrollY);
                    const height = Math.round(rect.height);
                    const width = Math.round(rect.width);
                    
                    if (height < 50 || width < 100) return;
                    
                    const key = `${top}-${height}`;
                    if (!found.has(key) && height > 100) {
                        found.add(key);
                        
                        const className = getClassName(el);
                        
                        // ç”Ÿæˆå”¯ä¸€é€‰æ‹©å™¨
                        let uniqueSelector = selector;
                        if (el.id) {
                            uniqueSelector = `#${el.id}`;
                        } else if (className) {
                            // ä½¿ç”¨ç±»åç”Ÿæˆæ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
                            const classes = className.split(' ').filter(c => c.trim()).slice(0, 2);
                            if (classes.length > 0) {
                                uniqueSelector = `${el.tagName.toLowerCase()}.${classes.join('.')}`;
                            } else {
                                uniqueSelector = `${selector}:nth-of-type(${idx + 1})`;
                            }
                        } else {
                            uniqueSelector = `${selector}:nth-of-type(${idx + 1})`;
                        }
                        
                        sections.push({
                            tag: el.tagName.toLowerCase(),
                            className: className.slice(0, 100),
                            id: el.id || '',
                            selector: uniqueSelector,
                            top: top,
                            height: height,
                            bottom: top + height,
                            width: width
                        });
                    }
                });
            }
            
            sections.sort((a, b) => a.top - b.top);
            
            // åˆå¹¶é‡å sections
            const merged = [];
            for (const section of sections) {
                if (merged.length === 0) {
                    merged.push(section);
                } else {
                    const last = merged[merged.length - 1];
                    const overlap = Math.max(0, Math.min(last.bottom, section.bottom) - Math.max(last.top, section.top));
                    if (overlap / Math.min(last.height, section.height) > 0.3) {
                        last.bottom = Math.max(last.bottom, section.bottom);
                        last.height = last.bottom - last.top;
                    } else {
                        merged.push(section);
                    }
                }
            }
            
            return merged;
        }
    """)
    return sections


async def wait_for_images_in_viewport(page, viewport_height: int, scroll_y: int) -> bool:
    """ç­‰å¾…è§†å£å†…çš„å›¾ç‰‡åŠ è½½å®Œæˆ
    
    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        viewport_height: è§†å£é«˜åº¦
        scroll_y: å½“å‰æ»šåŠ¨ä½ç½®
        
    Returns:
        bool: æ˜¯å¦æ‰€æœ‰å›¾ç‰‡éƒ½å·²åŠ è½½
    """
    max_wait_time = IMAGE_LOAD_CHECK_TIMEOUT_MS
    start_time = time.time() * 1000
    
    while (time.time() * 1000 - start_time) < max_wait_time:
        # æ£€æŸ¥è§†å£å†…çš„å›¾ç‰‡æ˜¯å¦åŠ è½½å®Œæˆ
        images_loaded = await page.evaluate(f"""
            () => {{
                const viewportTop = {scroll_y};
                const viewportBottom = viewportTop + {viewport_height};
                const images = Array.from(document.querySelectorAll('img'));
                const visibleImages = images.filter(img => {{
                    const rect = img.getBoundingClientRect();
                    const imgTop = rect.top + window.scrollY;
                    const imgBottom = imgTop + rect.height;
                    return (imgBottom >= viewportTop && imgTop <= viewportBottom);
                }});
                
                if (visibleImages.length === 0) return true;
                
                const loadedCount = visibleImages.filter(img => {{
                    // æ£€æŸ¥å›¾ç‰‡æ˜¯å¦åŠ è½½å®Œæˆ
                    if (img.complete && img.naturalHeight > 0) return true;
                    // æ£€æŸ¥æ˜¯å¦æœ‰data-srcç­‰æ‡’åŠ è½½å±æ€§
                    if (img.dataset.src || img.dataset.lazySrc) {{
                        // æ‡’åŠ è½½å›¾ç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»å¼€å§‹åŠ è½½
                        return img.src && img.src !== window.location.href;
                    }}
                    return false;
                }}).length;
                
                return loadedCount === visibleImages.length;
            }}
        """)
        
        if images_loaded:
            return True
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´åé‡è¯•
        await page.wait_for_timeout(IMAGE_LOAD_RETRY_DELAY_MS)
    
    return False


async def preload_lazy_content(page) -> None:
    """é¢„åŠ è½½æ‡’åŠ è½½å†…å®¹ï¼šæ»šåŠ¨æ•´é¡µè§¦å‘æ‰€æœ‰æ‡’åŠ è½½å’ŒåŠ¨ç”»
    
    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
    """
    print("   â³ é¢„åŠ è½½æ‡’åŠ è½½å†…å®¹...", flush=True)
    
    # è·å–é¡µé¢é«˜åº¦
    total_height = await page.evaluate("document.body.scrollHeight")
    viewport_height = await page.evaluate("window.innerHeight")
    
    # åˆ†æ®µæ»šåŠ¨ï¼Œè§¦å‘æ‡’åŠ è½½å’Œæ»šåŠ¨åŠ¨ç”»
    scroll_step = viewport_height // 2
    current_scroll = 0
    
    while current_scroll < total_height:
        await page.evaluate(f"window.scrollTo(0, {current_scroll})")
        await page.wait_for_timeout(SCROLL_WAIT_MS)
        # ç­‰å¾…å½“å‰è§†å£çš„å›¾ç‰‡åŠ è½½
        await wait_for_images_in_viewport(page, viewport_height, current_scroll)
        current_scroll += scroll_step
        total_height = await page.evaluate("document.body.scrollHeight")
    
    # æ»šåŠ¨åˆ°åº•éƒ¨
    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
    await page.wait_for_timeout(LAZY_LOAD_WAIT_MS)
    await wait_for_images_in_viewport(page, viewport_height, total_height - viewport_height)
    
    # å†æ¬¡ä»å¤´åˆ°å°¾æ»šåŠ¨ä¸€éï¼Œç¡®ä¿æ‰€æœ‰æ»šåŠ¨è§¦å‘çš„åŠ¨ç”»éƒ½æ‰§è¡Œ
    print("   â³ è§¦å‘æ»šåŠ¨åŠ¨ç”»...", flush=True)
    await page.evaluate("window.scrollTo(0, 0)")
    await page.wait_for_timeout(SCROLL_WAIT_MS)
    
    current_scroll = 0
    while current_scroll < total_height:
        await page.evaluate(f"window.scrollTo(0, {current_scroll})")
        await page.wait_for_timeout(ANIMATION_WAIT_MS)
        await wait_for_images_in_viewport(page, viewport_height, current_scroll)
        current_scroll += scroll_step
    
    # æ»šåŠ¨å›é¡¶éƒ¨
    await page.evaluate("window.scrollTo(0, 0)")
    await page.wait_for_timeout(LAZY_LOAD_WAIT_MS)
    await wait_for_images_in_viewport(page, viewport_height, 0)
    
    # ç­‰å¾…æ‰€æœ‰åŠ¨ç”»å®Œæˆï¼ˆæ•°å­—è®¡æ•°å™¨ã€æ·¡å…¥ç­‰ï¼‰
    await page.wait_for_timeout(ANIMATION_COMPLETE_WAIT_MS)
    
    print("   âœ… æ‡’åŠ è½½å’ŒåŠ¨ç”»å·²å®Œæˆ", flush=True)


async def capture_screenshots(page, output_name: str) -> Tuple[dict, list]:
    """æ™ºèƒ½åˆ†å—æˆªå›¾
    
    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆå·²æ¸…ç†ï¼‰
    
    Returns:
        tuple: (page_infoå­—å…¸, screenshotsåˆ—è¡¨)
    """
    viewport_height = Config.VIEWPORT_HEIGHT
    overlap = Config.OVERLAP
    
    # é¢„åŠ è½½æ‡’åŠ è½½å†…å®¹
    await preload_lazy_content(page)
    
    # è·å–é¡µé¢ä¿¡æ¯ï¼ˆæ‡’åŠ è½½åçš„çœŸå®é«˜åº¦ï¼‰
    page_info = await page.evaluate("""
        () => ({
            title: document.title,
            totalHeight: document.body.scrollHeight
        })
    """)
    
    total_height = page_info['totalHeight']
    
    # æ£€æµ‹ DOM ç»“æ„ï¼ˆä»…ä¾›å‚è€ƒï¼‰
    print("   ğŸ” æ£€æµ‹DOMç»“æ„ï¼ˆä»…ä¾›å‚è€ƒï¼‰...", flush=True)
    sections = await detect_dom_sections(page)
    
    screenshots = []
    
    # å§‹ç»ˆä½¿ç”¨åƒç´ åˆ†å—ï¼Œç¡®ä¿å®Œæ•´è¦†ç›–ï¼Œä¸é—æ¼è½®æ’­å›¾ç­‰åŠ¨æ€å†…å®¹
    method = 'pixel'

    # åƒç´ åˆ†å— - ç¡®ä¿ä»ä¸Šåˆ°ä¸‹å®Œæ•´æˆªå–ï¼Œä¸é—æ¼ä»»ä½•å†…å®¹
    print(f"   ğŸ“¸ ä½¿ç”¨åƒç´ åˆ†å—ï¼Œç¡®ä¿å®Œæ•´è¦†ç›–é¡µé¢ï¼ˆæ€»é«˜åº¦: {total_height}pxï¼‰", flush=True)

    chunk_height = viewport_height - overlap
    # è®¡ç®—æ‰€éœ€åˆ†å—æ•°ï¼Œç¡®ä¿å®Œæ•´è¦†ç›–
    num_chunks = max(1, (total_height + chunk_height - 1) // chunk_height)

    # å¦‚æœåˆ†å—å¤ªå¤šï¼Œç»™å‡ºè­¦å‘Šä½†ç»§ç»­
    if num_chunks > 20:
        print(f"   âš ï¸  é¡µé¢è¾ƒé•¿ï¼Œå°†ç”Ÿæˆ {num_chunks} ä¸ªåˆ†å—", flush=True)

    print(f"   ğŸ“Š åˆ†å—å‚æ•°: è§†å£é«˜åº¦={viewport_height}px, é‡å ={overlap}px, åˆ†å—é«˜åº¦={chunk_height}px", flush=True)

    for i in range(num_chunks):
        scroll_y = i * chunk_height

        # æ»šåŠ¨åˆ°ä½ç½®
        await page.evaluate(f"window.scrollTo(0, {scroll_y})")

        # ç­‰å¾…æ»šåŠ¨å®Œæˆ
        await page.wait_for_timeout(max(Config.LAZY_LOAD_SCROLL_WAIT, SCROLL_WAIT_MS))

        # ç­‰å¾…è§†å£å†…çš„å›¾ç‰‡åŠ è½½å®Œæˆ
        print(f"   â³ ç­‰å¾…åˆ†å— {i+1}/{num_chunks} çš„å›¾ç‰‡åŠ è½½...", flush=True)
        images_loaded = await wait_for_images_in_viewport(page, viewport_height, scroll_y)
        if not images_loaded:
            print(f"   âš ï¸  åˆ†å— {i+1} éƒ¨åˆ†å›¾ç‰‡å¯èƒ½æœªå®Œå…¨åŠ è½½ï¼Œç»§ç»­æˆªå›¾...", flush=True)

        # é¢å¤–ç­‰å¾…åŠ¨æ€å†…å®¹ï¼ˆè½®æ’­å›¾åˆ‡æ¢ã€åŠ¨ç”»ç­‰ï¼‰
        if i == 0:  # ç¬¬ä¸€å±å¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´åŠ è½½è½®æ’­å›¾
            await page.wait_for_timeout(FIRST_SCREEN_WAIT_MS)
            # å†æ¬¡æ£€æŸ¥ç¬¬ä¸€å±çš„å›¾ç‰‡
            await wait_for_images_in_viewport(page, viewport_height, 0)
        else:
            # å…¶ä»–åˆ†å—ä¹Ÿé¢å¤–ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿æ‡’åŠ è½½å›¾ç‰‡æœ‰æ—¶é—´åŠ è½½
            await page.wait_for_timeout(LAZY_LOAD_WAIT_MS)

        # ç­‰å¾…ç½‘ç»œç©ºé—²ï¼ˆç¡®ä¿æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚ï¼‰
        try:
            await page.wait_for_load_state("networkidle", timeout=2000)
        except Exception:
            pass  # å¦‚æœè¶…æ—¶ï¼Œç»§ç»­æˆªå›¾

        chunk_path = Config.SCREENSHOT_DIR / f"{output_name}_pixel_{i+1}.jpg"
        await page.screenshot(path=str(chunk_path), type='jpeg', quality=Config.SCREENSHOT_QUALITY)

        # è®¡ç®—å®é™…è¦†ç›–èŒƒå›´
        chunk_bottom = min(scroll_y + viewport_height, total_height)

        screenshots.append({
            'path': str(chunk_path),
            'scroll_y': scroll_y,
            'scroll_bottom': chunk_bottom,
            'method': 'pixel'
        })
        print(f"   âœ… åƒç´ åˆ†å— {i+1}/{num_chunks} (è¦†ç›–: {scroll_y}px - {chunk_bottom}px)", flush=True)

    # éªŒè¯æ˜¯å¦å®Œæ•´è¦†ç›–
    last_bottom = screenshots[-1].get('scroll_bottom', 0) if screenshots else 0
    coverage = (last_bottom / total_height * 100) if total_height > 0 else 0
    print(f"   ğŸ“Š è¦†ç›–èŒƒå›´: {last_bottom}/{total_height}px ({coverage:.1f}%)", flush=True)

    if coverage < 95:
        print(f"   âš ï¸  è­¦å‘Š: é¡µé¢è¦†ç›–ä¸å®Œæ•´ï¼Œå¯èƒ½é—æ¼äº† {total_height - last_bottom}px å†…å®¹", flush=True)
    
    return {
        'title': page_info['title'],
        'total_height': total_height,
        'screenshots': screenshots,
        'method': method,
        'dom_sections': sections
    }, screenshots


async def analyze_page(url: str, output_name: Optional[str] = None) -> dict:
    """åˆ†æé¡µé¢ä¸»å‡½æ•°
    
    Args:
        url: è¦åˆ†æçš„é¡µé¢URL
        output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»URLæå–ï¼‰
        
    Returns:
        dict: åˆ†ææŠ¥å‘Šå­—å…¸
    """
    # å¤„ç†è¾“å‡ºæ–‡ä»¶å
    if not output_name:
        raw_name = url.rstrip('/').split('/')[-1] or 'homepage'
    else:
        raw_name = output_name
    
    # æ¸…ç†æ–‡ä»¶å
    output_name = sanitize_filename(raw_name)
    
    print("=" * 60, flush=True)
    print("ğŸš€ é¡µé¢åˆ†æï¼ˆWordPressä¼˜åŒ–ç‰ˆï¼‰", flush=True)
    print(f"   URL: {url}", flush=True)
    print(f"   è¾“å‡ºåç§°: {output_name}", flush=True)
    print(f"   è§†è§‰æ¨¡å‹: {Config.VISION_MODEL}", flush=True)
    print("=" * 60, flush=True)
    
    # æ¸…ç©º output ç›®å½•ï¼ˆä¿ç•™ archiveï¼‰
    print("\nğŸ§¹ æ¸…ç©º output ç›®å½•ï¼ˆä¿ç•™ archiveï¼‰...", flush=True)
    deleted_files, deleted_size = Config.cleanup_output(keep_archive=True)
    if deleted_files > 0:
        print(f"   âœ… å·²åˆ é™¤ {deleted_files} ä¸ªæ–‡ä»¶ ({deleted_size / (1024*1024):.2f} MB)", flush=True)
    else:
        print(f"   â„¹ï¸  output ç›®å½•å·²ä¸ºç©º", flush=True)
    
    Config.ensure_dirs()
    
    # Step 1: æˆªå›¾
    print(f"\nğŸ“¸ æ™ºèƒ½æˆªå›¾: {url}", flush=True)
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = None
            page = None
            try:
                context = await browser.new_context(viewport={
                    'width': Config.VIEWPORT_WIDTH,
                    'height': Config.VIEWPORT_HEIGHT
                })
                page = await context.new_page()
                await page.goto(url, wait_until='networkidle', timeout=60000)
                await page.wait_for_timeout(ANIMATION_COMPLETE_WAIT_MS)

                page_info, screenshots = await capture_screenshots(page, output_name)
            finally:
                if page:
                    await page.close()
                if context:
                    await context.close()
            await browser.close()
    except Exception as e:
        print(f"\nâŒ æˆªå›¾å¤±è´¥: {e}", flush=True)
        raise
    
    # Step 2: è§†è§‰åˆ†æ
    print(f"\nğŸ¨ åˆ†æé¡µé¢ç»“æ„...", flush=True)
    
    analyzer = VisionAnalyzer()
    chunks_analysis = []
    
    for i, screenshot_info in enumerate(screenshots):
        screenshot_path = screenshot_info['path']
        section_info = screenshot_info.get('section')
        
        # è·å–ä¸Šä¸‹æ–‡æˆªå›¾è·¯å¾„ï¼ˆä»ç¬¬äºŒå¼ å¼€å§‹æä¾›ä¸Šä¸‹æ–‡ï¼‰
        prev_image_path = None
        next_image_path = None
        
        if i > 0:  # ä»ç¬¬äºŒå¼ å¼€å§‹ï¼Œæä¾›ä¸Šä¸€å¼ ä½œä¸ºä¸Šä¸‹æ–‡
            prev_image_path = screenshots[i-1]['path']
        
        if i < len(screenshots) - 1:  # å¦‚æœä¸æ˜¯æœ€åä¸€å¼ ï¼Œæä¾›ä¸‹ä¸€å¼ ä½œä¸ºä¸Šä¸‹æ–‡
            next_image_path = screenshots[i+1]['path']
        
        context_info = ""
        if prev_image_path or next_image_path:
            context_parts = []
            if prev_image_path:
                context_parts.append("ä¸Šä¸€å¼ ")
            if next_image_path:
                context_parts.append("ä¸‹ä¸€å¼ ")
            context_info = f"ï¼ˆå«{'å’Œ'.join(context_parts)}ä¸Šä¸‹æ–‡ï¼‰"
        
        print(f"   [{i+1}/{len(screenshots)}] åˆ†æåˆ†å—{context_info}...", flush=True)
        try:
            analysis = analyzer.analyze_chunk(
                screenshot_path, i, len(screenshots), section_info,
                prev_image_path=prev_image_path,
                next_image_path=next_image_path
            )
            chunks_analysis.append(analysis)
            module_type = analysis.get('module_type', 'unknown')
            print(f"       âœ… {module_type}", flush=True)
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­åˆ†æ", flush=True)
            raise
        except Exception as e:
            error_msg = str(e)
            print(f"       âŒ åˆ†æå¤±è´¥: {error_msg[:100]}", flush=True)
            chunks_analysis.append({
                "error": error_msg,
                "screenshot": screenshot_path,
                "chunk_index": i
            })
        
        # é¿å…APIé™æµï¼ˆæœ€åä¸€ä¸ªåˆ†å—ä¸éœ€è¦ç­‰å¾…ï¼‰
        if i < len(screenshots) - 1:
            await asyncio.sleep(API_RATE_LIMIT_DELAY)
    
    # Step 3: ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“¦ ç”Ÿæˆåˆ†ææŠ¥å‘Š...", flush=True)
    
    # ä¿å­˜JSONåˆ†æç»“æœ
    analysis_report = {
        "url": url,
        "title": page_info['title'],
        "total_height": page_info['total_height'],
        "method": page_info['method'],
        "vision_model": Config.VISION_MODEL,
        "screenshots": [s['path'] for s in screenshots],
        "chunks": chunks_analysis
    }
    
    # ç”Ÿæˆå¸¦æ¨¡å‹åç¼€çš„æ–‡ä»¶å
    model_suffix = Config.get_model_suffix()
    json_path = Config.ANALYSIS_DIR / f"{output_name}_{model_suffix}.json"
    
    try:
        json_path.write_text(
            json.dumps(analysis_report, ensure_ascii=False, indent=2), 
            encoding='utf-8'
        )
        print(f"   âœ… JSONæŠ¥å‘Šå·²ä¿å­˜", flush=True)
    except Exception as e:
        print(f"   âš ï¸  JSONä¿å­˜å¤±è´¥: {e}", flush=True)
    
    # ç”ŸæˆCursoræç¤ºæ–‡ä»¶
    prompt_path = None
    try:
        prompt_content = generate_cursor_prompt_file(
            url=url,
            title=page_info['title'],
            total_height=page_info['total_height'],
            chunks=chunks_analysis,
            screenshot_dir=Config.SCREENSHOT_DIR,
            output_name=output_name
        )

        prompt_path = Config.ANALYSIS_DIR / f"{output_name}_{model_suffix}_prompt.md"
        prompt_path.write_text(prompt_content, encoding='utf-8')
        print(f"   âœ… Promptæ–‡ä»¶å·²ä¿å­˜", flush=True)
    except Exception as e:
        print(f"   âš ï¸  Promptç”Ÿæˆå¤±è´¥: {e}", flush=True)
    
    print("\n" + "=" * 60, flush=True)
    print("âœ… åˆ†æå®Œæˆ!", flush=True)
    print(f"   ğŸ“¸ æˆªå›¾: {Config.SCREENSHOT_DIR}/{output_name}_*.jpg", flush=True)
    print(f"   ğŸ“Š åˆ†æ: {json_path}", flush=True)
    if prompt_path:
        print(f"   ğŸ“ Prompt: {prompt_path}", flush=True)
    print(f"   ğŸ”§ æ–¹æ³•: {page_info['method']}", flush=True)
    print(f"   ğŸ“Š åˆ†å—æ•°: {len(screenshots)}", flush=True)
    print(f"   âœ… æˆåŠŸåˆ†æ: {sum(1 for c in chunks_analysis if 'error' not in c)}/{len(chunks_analysis)}", flush=True)
    print("=" * 60, flush=True)

    if prompt_path:
        print("\n" + "=" * 60, flush=True)
        print("ğŸ‘‰ ä¸‹ä¸€æ­¥ï¼šåœ¨Cursorä¸­ä½¿ç”¨", flush=True)
        print(f"   @{prompt_path.name} @output/screenshots æ ¹æ®åˆ†æç”ŸæˆWordPressæ¨¡å—ä»£ç ", flush=True)
        print("=" * 60, flush=True)
    
    # å½’æ¡£æ–‡ä»¶
    archive_files(url, output_name)
    
    return analysis_report


async def main() -> None:
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python scripts/analyze.py <url> [output_name]")
        print("\nç¤ºä¾‹:")
        print("  python scripts/analyze.py https://example.com/products")
        print("  python scripts/analyze.py https://example.com/products my-products")
        sys.exit(1)
    
    url = sys.argv[1]
    output_name = sys.argv[2] if len(sys.argv) > 2 else None
    
    # éªŒè¯URLæ ¼å¼
    if not url.startswith(('http://', 'https://')):
        print(f"âŒ æ— æ•ˆçš„URLæ ¼å¼: {url}")
        print("   è¯·ä½¿ç”¨å®Œæ•´çš„URLï¼Œä¾‹å¦‚: https://example.com")
        sys.exit(1)
    
    # éªŒè¯é…ç½®
    errors = Config.validate()
    if errors:
        print("\nâŒ é…ç½®é”™è¯¯:")
        for error in errors:
            print(f"   - {error}")
        print("\nè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ç›¸å…³é…ç½®")
        sys.exit(1)
    
    try:
        await analyze_page(url, output_name)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ", flush=True)
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}", flush=True)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
    